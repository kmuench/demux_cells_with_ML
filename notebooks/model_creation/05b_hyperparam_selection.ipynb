{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter selection\n",
    "\n",
    "**Goal**: Optimize settings/parameters within a chosen model class to maximize performance.\n",
    "\n",
    "**Output**: A report with metrics to inform which model types are likely to perform better.\n",
    "\n",
    "**Notes**:\n",
    "- Even though logistic regression using all of the FPR (All) genes did sliiightly better, there are so many more features and I have so few cells that I'm worried about creating issues when N features >> N cells\n",
    "\n",
    "**TODO**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import needed libraries\n",
    "import scanpy as sc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils.config import *\n",
    "from utils.analysis_variables import *\n",
    "from utils.analysis_functions import *\n",
    "\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import make_scorer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scanpy setup\n",
    "sc.settings.verbosity = 3 # corresponds to hints\n",
    "\n",
    "# Notebook setup\n",
    "np.random.seed(15)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important paths\n",
    "notebook_name = \"05b_hyperparam_selection\"\n",
    "\n",
    "# path_outdir_base = \"../../output/20240221_import\"\n",
    "path_results = os.path.join(path_outdir_base, notebook_name)\n",
    "os.makedirs(path_results, exist_ok=True)\n",
    "\n",
    "path_input_data = os.path.join(path_outdir_base, \"03_create_test_train\", \"training_data.pkl\")\n",
    "path_input_adata = os.path.join(path_outdir_base, \"03_create_test_train\", \"adata_labeled.h5ad\")\n",
    "path_input_features = os.path.join(path_outdir_base, \"04_feature_selection\", \"dict_feature_data.pkl\")\n",
    "\n",
    "chosen_feature_set = \"FPR (200)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_input_data, 'rb') as f:\n",
    "    data_dict = pickle.load(f)\n",
    "\n",
    "X_train = data_dict['X']\n",
    "y_train = data_dict['Y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_labeled = sc.read_h5ad(path_input_adata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Highly Variable Genes', 'K Best (100)', 'K Best (200)', 'K Best (Signif)', 'FPR (All)', 'FPR (200)'])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open(path_input_features, 'rb') as file:\n",
    "    dict_feature_data = pickle.load(file)\n",
    "\n",
    "print(dict_feature_data.keys())\n",
    "prelim_model_features = dict_feature_data[chosen_feature_set]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (7407, 266), Shape of Y_train: (7407,)\n"
     ]
    }
   ],
   "source": [
    " # Subset training data to only include X_data corresponding to features we want to use\n",
    "adata_model = adata_labeled[:, adata_labeled.var.index.isin(prelim_model_features)]\n",
    "\n",
    "mask_features = adata_labeled.var.index.isin(prelim_model_features)\n",
    "X_train_filtered = X_train[:, mask_features]\n",
    "\n",
    "print(f\"Shape of X_train: {X_train_filtered.shape}, Shape of Y_train: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train models using a wider set of hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define scoring metrics\n",
    "\n",
    "scoring = {\n",
    "    'accuracy': 'accuracy',  # Default scoring for classification\n",
    "    'specificity': make_scorer(specificity),\n",
    "    'false_positive_rate': make_scorer(false_positive_rate, greater_is_better=False),  # Minimize false positive rate\n",
    "    'false_negative_rate': make_scorer(false_negative_rate, greater_is_better=False),  # Minimize false negative rate; something funny abt this processing makes this neg\n",
    "    'precision': make_scorer(precision)\n",
    "}\n",
    "\n",
    "# Define pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', None), # classifier will be replaced during grid search\n",
    "])\n",
    "\n",
    "param_grid_svc = {\n",
    "    'classifier': [SVC()],\n",
    "    'classifier__C': [0.01, 0.1, 1, 10, 100],  # Regularization parameter; smaller is stronger\n",
    "    'classifier__kernel': ['rbf', 'linear', 'poly', 'sigmoid'],\n",
    "    'classifier__gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "param_grid_lr = {\n",
    "    'classifier': [LogisticRegression()],\n",
    "    'classifier__C': [0.01, 0.1, 1, 10, 100],  # Regularization parameter; smaller is stronger\n",
    "    'classifier__penalty': ['l1', 'l2', 'elasticnet', None],\n",
    "    'classifier__solver': ['lbfgs', 'liblinear']\n",
    "}\n",
    "\n",
    "# Combine the param grids into a list\n",
    "all_param_grids = [param_grid_svc, param_grid_lr]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do grid search CV on different parameter sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GridSearchCV_on_featureset(X_train, featureset_id, all_param_grids=all_param_grids, pipeline=pipeline, scoring=scoring, path_results=path_results):\n",
    "    print(f\"RandomizedSearchCV on {featureset_id}\")\n",
    "    \n",
    "    # fit the model and assess\n",
    "    best_params_list = []\n",
    "    best_estimator_list = []\n",
    "    cv_result_list = []\n",
    "    cv_result_list_df = []\n",
    "    cv_result_list_filtered = []\n",
    "\n",
    "    for param_grid in all_param_grids:\n",
    "        grid_search = RandomizedSearchCV(pipeline, param_grid, cv=5, scoring=scoring, refit='accuracy') #GridSearchCV\n",
    "        grid_search.fit(X_train, y_train)\n",
    "\n",
    "        # Access the best parameters and best estimator for each classifier\n",
    "        best_params_list.append( grid_search.best_params_ )\n",
    "        best_estimator_list.append(grid_search.best_estimator_)\n",
    "\n",
    "        cv_results = grid_search.cv_results_\n",
    "        cv_result_list.append(cv_results)\n",
    "\n",
    "        print(cv_results['mean_test_accuracy'])\n",
    "\n",
    "        print(f\"Best Parameters: {grid_search.best_params_ }\")\n",
    "        print(f\"Best Estimator: {grid_search.best_estimator_}\")\n",
    "\n",
    "\n",
    "    for cv_scores in cv_result_list:\n",
    "        cv_df = pd.DataFrame.from_dict(cv_scores)\n",
    "\n",
    "        cols_params = [x for x in cv_df.columns if x.startswith(\"param\")]\n",
    "        cols_scores =  [x for x in cv_df.columns if x.startswith(\"mean_\")]\n",
    "\n",
    "        cv_result_list_df.append(cv_df)\n",
    "        cv_result_list_filtered.append(cv_df[cols_params+cols_scores])\n",
    "\n",
    "    merged_cv_df = pd.concat(cv_result_list_df, join='outer', axis=0)\n",
    "    merged_cv_df_filtered = pd.concat(cv_result_list_filtered, join='outer', axis=0)\n",
    "\n",
    "    merged_cv_df.to_csv(os.path.join(path_results, 'merged_cv_df_' + featureset_id+'.csv'))\n",
    "    merged_cv_df_filtered.to_csv(os.path.join(path_results, 'merged_cv_df_filtered_' + featureset_id + '.csv'))\n",
    "\n",
    "    return(merged_cv_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomizedSearchCV on FPR (200)\n",
      "[0.96989278 0.98015347 0.98015347 0.94343137 0.98096364 0.98096364\n",
      " 0.83353745 0.91224589 0.93816557 0.96827243]\n",
      "Best Parameters: {'classifier__kernel': 'rbf', 'classifier__gamma': 'scale', 'classifier__C': 10, 'classifier': SVC()}\n",
      "Best Estimator: Pipeline(steps=[('scaler', StandardScaler()), ('classifier', SVC(C=10))])\n",
      "[       nan        nan 0.97704883 0.97286401 0.97164898        nan\n",
      " 0.97556334 0.95139723        nan 0.97691387]\n",
      "Best Parameters: {'classifier__solver': 'liblinear', 'classifier__penalty': 'l2', 'classifier__C': 0.1, 'classifier': LogisticRegression()}\n",
      "Best Estimator: Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('classifier', LogisticRegression(C=0.1, solver='liblinear'))])\n"
     ]
    }
   ],
   "source": [
    "result = GridSearchCV_on_featureset(X_train_filtered, chosen_feature_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Based on results, create the best model\n",
    "\n",
    "SVMs are less interpretable than logistic regression models, but importance of features can still be hinted at by looking at how far genes fall from decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(C=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(C=1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(C=1)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = SVC(C=1, kernel = 'rbf', gamma = 'scale')\n",
    "best_model.fit(X_train_filtered, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# with open(os.path.join(path_results, 'training_data_selectfeatures.pkl'), 'wb') as f:\n",
    "#     pickle.dump({'X': X_train_features, 'Y': y_train}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../../output/20240221_import/05b_hyperparam_selection/best_model.pkl']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(best_model, os.path.join(path_results, 'best_model.pkl'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
